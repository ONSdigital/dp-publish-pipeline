#!/usr/bin/env bash

usage() {
	sed 's/^	//' <<EOF
	$(basename $0) <args...>
		--up		- start all the processes (default) (see --svc)
		--ps		- list all running processes
		--kill		- kill all running processes
		--restart	- do these: --kill --ps-ok --rotate --up (see --svc)
		--bounce	- do these: --restart --tail
		--svc "a b:N c"	- list of services (--{up,restart}) (N = opt no. of procs)
				  (currently: $SERVICES)

		--tail		- tail all logfiles
		--rotate	- rotate all logfiles
		--time [ext]	- show logs of main (first,last) events [logs with .ext]

		--seed collID [collID_end] [:offset_sec] [+stagger_sec]
				- show the JS messages to schedule collID[..collID_end]
					- start collID now (default) or next minute+<offset_sec>
					- start N+1th job <stagger_sec> after Nth (default=0)
		--unseed ...	- cancel scheduled jobs, args as per --seed
		--file		- when using --seed, use file://... rather than s3://....
		--seedlines	- when using --seed, use multiline
		-p		- run the kafka producer (e.g. dp --seed test0001|dp -p)

		--resize	- change the message size for kafka
		--groups	- show the kafka

		--db		- show the DB table 'schedule', 'schedule_file'
		--db-wipe	- clear the DB tables: $sql_tables
		--db-count	- show the count of rows in the --db-wipe tables, and docs in mongo
		--db-index	- ensure mongo indexes exist and show them
EOF
	[[ -n $1 ]] && exit $1
}

psql_args="-U dp dp"
os=$(uname -s)
source $HOME/.dprc
if [[ $os == Darwin ]]; then # it's a dev mac
	export ZEBEDEE_ROOT=${ZEBEDEE_ROOT:-./test-data}
	def_svc_num=${def_svc_num:-3}
	export MONGODB=${MONGODB:-localhost:27017}
	start_kafka=
	PATH=$PATH:$PWD/build/darwin-amd64
	k_ext=
	use_src=1
	p_flags=
else # aws
	export ZEBEDEE_ROOT=${ZEBEDEE_ROOT:-$HOME/test-data}
	def_svc_num=${def_svc_num:-20}
	# the below line assumes you update the IP with: sudo vi /etc/hosts
	export MONGODB=${MONGODB:-mongo:27017}
	# add kafka commands to PATH
	[[ -d /opt/kafka_2.10-0.10.1.0/bin ]] && PATH=$PATH:/opt/kafka_2.10-0.10.1.0/bin
	[[ -d ~kafka ]] && start_kafka=1
	# try to ensure commands (e.g. dp, publish-scheduler, ...) are on PATH
	which dp > /dev/null 2>&1 || PATH=$PATH:$HOME/bin
	k_ext=.sh
	use_src=
	p_flags=f
	if [[ -n $DB_ACCESS ]]; then
		psql_args=$DB_ACCESS
	fi
fi
export zebedee_root=$ZEBEDEE_ROOT KAFKA_MAX_BYTES=${KAFKA_MAX_BYTES:-10000000} MONGODB_DB=${MONGODB_DB:-onswebsite}
export ZOOKEEPER=${ZOOKEEPER:-localhost:2181} KAFKA_ADDR=${KAFKA_ADDR:-localhost:9092}

LOG_DIR=~/log
KAFKA_TOPIC=${KAFKA_TOPIC:-uk.gov.ons.dp.web.schedule} sched_group=publish-scheduler tmp_file=/tmp/${USER}_dp_$$
KAFKA_PARTITIONS=${KAFKA_PARTITIONS:-12}
UPSTREAM_S3_BUCKET=${UPSTREAM_S3_BUCKET:-upstream-content}
SERVICES=${SERVICES:-publish-metadata:$def_svc_num publish-data:$def_svc_num publish-receiver:$def_svc_num publish-tracker publish-scheduler}
sql_tables="schedule schedule_file schedule_delete s3data metadata"
use_mongo=
use_file=
seedlines=-n

colr_n() { local col=$1; shift; if [[ -t 0 ]]; then echo -e '\e['${col}m"$@"'\e[0m'; else echo "$@"; fi; }
colr() { colr_n 32 "$@"; }
log()  { echo $(colr_n 36 $(date '+%Y-%m-%d %H:%M:%S')) "$@"; }
warn() { log "$@" >&2; }
die()  { warn "$@"; exit 2; }
sql()      { psql $psql_args -c "$@"; }
sql_sh()   { psql $psql_args -t -c "$@"; }
mongo_js() { mongo $MONGODB/$MONGODB_DB --quiet --eval "$@"; }
# mongo_js() { echo "$@" > $tmp_file.js  || die Cannot write to $tmp_file.js;  mongo $MONGODB/$MONGODB_DB --quiet $tmp_file.js; rm -f $tmp_file.js; }
ensure_kafka() { ps -ef | grep -q ' java .* [k]afka' && return; warn No kafka - starting...; sudo -i -u kafka ~kafka/bin/start-kafka; }
# ensure_mongo() { sudo docker ps| grep -q 'mongo' && return;     warn No mongo - starting...; sudo docker start $MONGODB_DB; }
kick_off() { local n=$1 cmd=$2 l=${2%.sh}; shift 2; l=${l%.go}; l=${l##*-}
	# put bg commands into their own process group - so that killing this script doesn't kill $cmd
	set -o monitor
	for (( i=0; i<n; i++ )); do
		e=; if (( i > 0 )); then e=$i; fi
		# echo $cmd $LOG_DIR/$l$e.log
		nohup $cmd >> $LOG_DIR/$l$e.log 2>&1 &
	done
}

[[ -n $start_kafka ]] && ensure_kafka
# ensure_mongo
mkdir -p $LOG_DIR || exit 3

[[ -z $1 ]] && set -- --up
res=0

while [[ -n $1 ]]; do
	arg=$1; shift
	if   [[ $arg == -h || $arg == --help ]]; then usage; break
	elif [[ $arg == --svc     ]]; then SERVICES=$1; shift
	elif [[ $arg == --file    ]]; then use_file=1
	elif [[ $arg == --seedlines ]]; then seedlines=
	elif [[ $arg == --kill || $arg == --ps || $arg == --ps-ok ]]; then
		proc_re=
		for s in $SERVICES; do
			s=${s%:*}
			proc_re=$proc_re${proc_re:+|}$s
		done
		log ${arg#--}... $proc_re
		if   [[ $arg == --kill ]]; then
			if [[ $os == Darwin ]]; then	pkill -x "$proc_re"
			else				killall -r "^($proc_re)\$"
			fi
		elif [[ $arg == --ps || $arg == --ps-ok ]]; then
			pgrep -${p_flags}lx "$proc_re"; ps_res=$?
			[[ $arg == --ps ]] && res=$ps_res
		fi
	elif [[ $arg == --sleep   ]]; then log Sleeping to let logfiles be created...; sleep $1; shift
	elif [[ $arg == --tail    ]]; then log Tailing...; exec tail -f $LOG_DIR/*.log
	elif [[ $arg == --time    ]]; then
					ext=.log; if [[ -n $1 && $1 == [0-9]* ]]; then ext=.log.$1; shift; fi
					cd $LOG_DIR || exit 2
					(
						test -f scheduler.log && grep 'start:' scheduler$ext				|sort -n |tail -2
						test -f scheduler.log && grep 'completed' scheduler$ext				|sort -n |tail -2
						test -f scheduler.log && grep 'ERROR' scheduler$ext				|sort -n |tail -2
						test -f tracker.log   && grep Completed tracker$ext				|sort -n |tail -1
						test -f metadata.log  && tail -qn 1 metadata*$ext				|sort -n |tail -1
						test -f data.log      && tail -qn 1 data*$ext					|sort -n |tail -1
						test -f receiver.log  && tail -qn 15 receiver*$ext |egrep 'Added (S3|metadata)'	|sort -n |tail -1
					)|sort -n
	elif [[ $arg == --bounce  ]]; then set -- --kill --ps-ok --rotate --up --sleep 1 --tail "$@"
	elif [[ $arg == --restart ]]; then set -- --kill --ps-ok --rotate --up "$@"
	elif [[ $arg == --rotate  ]]; then date=$(date '+%Y%m%d-%H%M%S'); log Logs get extension .$date; for i in $LOG_DIR/*.log; do mv $i $i.$date; done
	elif [[ $arg == --seed || $arg == --unseed ]]; then
		when=0 coll= end_coll= stagger=0 when_pp=now
		while [[ -n $1 ]]; do
			if [[ -z $coll ]]; then
				coll=$1
				end_coll=$coll
			elif [[ $1 =~ "+"[0-9]+ ]]; then
				stagger=${1#+}
			elif [[ $1 =~ :[0-9]+ ]]; then
				if [[ $os == Darwin ]]; then # it's a dev mac
					when=$(date -v+1M -v${1#:}S '+%s')  # get time for next minute and $1 secs
					when_pp=$(date -r $when)
				else
					when=$(date -d "1 minute -$(date '+%S') seconds ${1#:} seconds" '+%s')  # get time for next minute:00 add $1 secs
					when_pp=$(date -d'@'$when)
				fi
			elif [[ -d $ZEBEDEE_ROOT/zebedee/collections/$1/complete ]]; then
				end_coll=$1
			else
				break
			fi
			shift
		done
		[[ -z $coll ]] && usage 1 >&2
		shopt -s extglob # for the regexes in the for() (remove leading zeroes otherwise bash thinks it is octal)
		count=0 count_all_files=
		for (( c=${coll##test*(0)}; c <= ${end_coll##test*(0)}; c++ )); do
			len=${#c} col=${coll:0:-len}$c       # get prefix of $coll removing length of $c (i.e. coll=test0009,c=10 so len=2 col=test0010)
			# echo c=$c len=$len col=$col
			files2del= filecount=0 coll_d=$ZEBEDEE_ROOT/zebedee/collections/$col/complete
			if [[ $arg == --seed ]]; then
				echo $seedlines '{"Action":"schedule","CollectionID":"'"CollId_$col"'","CollectionPath":"'"$col"'","ScheduleTime":"'$when'","Files":['
				cd $coll_d && find . -type f | while read i; do
					i=${i#\.}
					if [[ $i = /manifest.json ]]; then
						files2del=$(jq .urisToDelete $coll_d$i)
						[[ -n $files2del ]] && files2del=',"UrisToDelete":'"$files2del"
						continue
					fi
					(( filecount == 0 )) || echo -n ,
					if [[ -n $use_file ]]; then
						echo $seedlines '{"Location":"file://'"${coll_d}/$i"'","Uri":"'"$i"'"}'
					else
						echo $seedlines '{"Location":"s3://'"${UPSTREAM_S3_BUCKET}/$col$i"'","Uri":"'"$i"'"}'
					fi
					let filecount++
				done
				echo ']'"$files2del"'}'
			elif [[ $arg == --unseed ]]; then
				echo '{"Action":"cancel","CollectionID":"'"CollId_$col"'","ScheduleTime":"'$when'"}'
			fi
			(( stagger > 0 )) && let when+=stagger
			let count++
			[[ -n $count_all_files ]] && count_all_files=$count_all_files,
			count_all_files=$count_all_files${filecount}f
		done
		if [[ $arg == --unseed ]]; then
			warn cancelled $(colr $count) jobs from $(colr $when_pp) ... then $stagger sec apart
		else
			warn scheduled $(colr $count) jobs $count_all_files from $(colr $when_pp) ... then $stagger sec apart
		fi
	elif [[ $arg == -m ]]; then
		sudo docker run -it --link $MONGODB_DB:mongo --rm mongo sh -c 'exec mongo "$MONGO_PORT_27017_TCP_ADDR:$MONGO_PORT_27017_TCP_PORT/'"$MONGODB_DB"'"'
	elif [[ $arg == -c ]]; then log Starting consumer... of $KAFKA_TOPIC; kafka-console-consumer${k_ext} --bootstrap-server $KAFKA_ADDR --topic $KAFKA_TOPIC ${KAFKA_OFFSET:+--partition 0 --offset $KAFKA_OFFSET}
	elif [[ $arg == -p ]]; then
		log Starting producer... to $KAFKA_TOPIC
		kafka-console-producer${k_ext} --broker-list $KAFKA_ADDR --topic $KAFKA_TOPIC --producer-property max.request.size=$KAFKA_MAX_BYTES --property max.message.bytes=$KAFKA_MAX_BYTES
	elif [[ $arg == --resize ]]; then
		for c in uk.gov.ons.dp.web.complete-file uk.gov.ons.dp.web.schedule; do
			log "Setting max size of $KAFKA_MAX_BYTES for $c"
			kafka-configs$k_ext --zookeeper $ZOOKEEPER --entity-type topics --entity-name $c --alter --add-config max.message.bytes=$KAFKA_MAX_BYTES
		done
	elif [[ $arg == --repartition ]]; then
		kafka-topics$k_ext --alter --zookeeper $ZOOKEEPER --partitions $KAFKA_PARTITIONS --topic $KAFKA_TOPIC
	elif [[ $arg == --groups ]]; then
		for i in $(kafka-consumer-groups$k_ext --bootstrap-server $KAFKA_ADDR --list);do log $i; kafka-consumer-groups$k_ext --new-consumer --bootstrap-server $KAFKA_ADDR --group $i --describe 2>&1|egrep -v '^(Note: This will only.*)?$';done
	elif [[ $arg == --reset ]]; then
		log "Starting all-consuming consumer of $sched_group group for $KAFKA_TOPIC (Ctrl-C to quit)..."
		kafka-verifiable-consumer$k_ext --broker-list $KAFKA_ADDR --topic $KAFKA_TOPIC --group-id $sched_group --enable-autocommit --reset-policy latest --max-messages 1
	elif [[ $arg == --db || $arg == --db-all ]]; then
		sql "select * from schedule order by schedule_id" | perl -MTime::Piece -pE 'if($.<3){}else{s/\b(\d{13,})\b/$1 . " " . localtime->new(substr($1,0,-9))->datetime/eg;}'
		for t in schedule_file schedule_delete; do
			sql "select schedule_id, count(complete_time) AS ${t}_complete   from $t where complete_time IS NOT NULL group by schedule_id order by schedule_id"
			sql "select schedule_id, count(complete_time) AS ${t}_incomplete from $t where complete_time IS NULL     group by schedule_id order by schedule_id"
		done
		if [[ $arg == --db-all ]]; then
			for t in schedule_file schedule_delete; do
				log Incomplete from table: $t
				sql "select * from $t where complete_time IS NULL order by schedule_id,${t}_id" | perl -MTime::Piece -pE 'if($.<3){}else{s/\b(\d{13,})\b/$1 . " " . localtime->new(substr($1,0,-9))->datetime/eg;}'
			done
			sql "select * from s3data"
			sql "select * from metadata"
		fi
	elif [[ $arg == --db-wipe  ]]; then
		for i in $sql_tables; do echo -n "$i  	"; sql "delete from $i"; done
		[[ -n $use_mongo ]] && mongo_js 'print(JSON.stringify(db.s3.remove({}),null,8));db.meta.remove({});'
	elif [[ $arg == --db-count ]]; then
		sql_counts=$(for i in $sql_tables; do echo $i:$(sql_sh "select count(*) from $i");done); [[ -z $sql_counts ]] && die Could not count DB
		m_out=;if [[ -n $use_mongo ]]; then m_out=$(mongo_js "print(db.s3.count(), db.meta.count());"); [[ -z $m_out ]] && die Bad mongo count; m_out="MONGO s3:${m_out% *} meta:${m_out#* } total: $(( ${m_out/ /+} ))"; fi
		log SQL: $sql_counts $m_out
	elif [[ $arg == --db-index ]]; then
		[[ -n $use_mongo ]] || die Mongo not enabled: use_mongo=$use_mongo
		mongo_js 'print(JSON.stringify(db.s3.createIndex({fileLocation:1}), null, 8),JSON.stringify(db.meta.createIndex({fileLocation:1,language:1}), null, 8))'
		# colls_js='var colls=["s3","meta"];' for_colls_js="$colls_js"'for(len=colls.length, i=0; i<len; i++)'
		# mongo_js "$for_colls_js"'{print(JSON.stringify(db[colls[i]].getIndexes(), null, 8))}'
		mongo_js 'print(JSON.stringify(db.s3.getIndexes(), null, 8),JSON.stringify(db.meta.getIndexes(), null, 8))'
	elif [[ $arg == --up ]]; then
		log Starting all services...
		for s in $SERVICES; do
			num=1
			if [[ ${s##*:} != $s ]]; then
				num=${s##*:}
				s=${s%:*}
			fi
			sh=~/bin/run-$s.sh
			if [[ -n $use_src ]]; then
				kick_off $num "go run -race $s/$s.go"
			elif [[ -x $sh ]]; then
				kick_off $num $sh
			else
				kick_off $num $s
			fi
		done
	else
		usage >&2
		die Bad arg: $arg
	fi
done

exit $res
# vim: tabstop=8 softtabstop=0 noexpandtab smarttab smartindent
