#!/usr/bin/env bash

usage() {
	sed 's/^	//' <<EOF
	$(basename $0) <args...>
		--up		- start all the processes (default) (see --svc)
		--ps		- list all running processes
		--kill		- kill all running processes
		--restart	- do these: --kill --ps --rotate --up --tail (see --svc)
		--svc "a b:N c"	- list of services (--{up,restart}) (N = opt no. of procs)
				  (currently: $SERVICES)

		--tail		- tail all logfiles
		--rotate	- rotate all logfiles
		--time [ext]	- show logs of main (first,last) events [logs with .ext]

		--seed collID [collID_end] [:offset_sec] [+stagger_sec]
				- show the JS messages to schedule collID[..collID_end]
					- start collID now (default) or next minute+<offset_sec>
					- start N+1th job <stagger_sec> after Nth (default=0)
		-p		- run the kafka producer (e.g. dp --seed test0001|dp -p)

		--resize	- change the message size for kafka
		--groups	- show the kafka

		--db		- show the DB table kschedule'
		--db-wipe	- clear the DB table 'schedule'
		--db-count	- show the count of rows in 'schedule', and docs in mongo
		--db-index	- ensure mongo indexes exist and show them
EOF
	[[ -n $1 ]] && exit $1
}

os=$(uname -s)
if [[ $os == Darwin ]]; then # it's a dev mac
	export ZEBEDEE_ROOT=${ZEBEDEE_ROOT:-./test-data}
	def_svc_num=3
	export MONGODB=${MONGODB:-localhost:27017}
	start_kafka=
	PATH=$PATH:$PWD/build/darwin-amd64
	k_ext=
	use_src=1
	p_flags=
else # aws
	export ZEBEDEE_ROOT=${ZEBEDEE_ROOT:-$HOME/test-data}
	def_svc_num=20
	# the below line assumes you update the IP with: sudo vi /etc/hosts
	export MONGODB=${MONGODB:-mongo:27017}
	# add kafka commands to PATH
	PATH=$PATH:/opt/kafka_2.10-0.10.1.0/bin
	[[ -d ~kafka ]] && start_kafka=1
	k_ext=.sh
	use_src=
	p_flags=f
	if [[ -n $DB_ACCESS ]]; then
		psql_args=${DB_ACCESS%/*} # remove /dbname
		psql_args="--host ${psql_args#*@}" # remove postgresql://user@
	fi
fi
export zebedee_root=$ZEBEDEE_ROOT KAFKA_MAX_BYTES=${KAFKA_MAX_BYTES:-10000000} MONGODB_DB=${MONGODB_DB:-onswebsite}
export ZOOKEEPER=${ZOOKEEPER:-localhost:2181} KAFKA_ADDR=${KAFKA_ADDR:-localhost:9092}

LOG_DIR=~/log
sched_topic=uk.gov.ons.dp.web.schedule sched_group=publish-scheduler tmp_file=/tmp/${USER}_dp_$$
SERVICES=${SERVICES:-publish-sender:$def_svc_num static-content-migrator:$def_svc_num publish-receiver:$def_svc_num publish-tracker publish-scheduler}

colr() { echo -e '\e[32m'"$@"'\e[0m'; }
log()  { echo -e '\e[36m'$(date '+%Y-%m-%d %H:%M:%S')'\e[0m' "$@"; }
warn() { log "$@" >&2; }
die()  { warn "$@"; exit 2; }
sql()      { echo "$@" > $tmp_file.sql || die Cannot write to $tmp_file.sql; psql $psql_args -U dp dp -f $tmp_file.sql;      rm -f $tmp_file.sql; }
mongo_js() {  mongo $MONGODB/$MONGODB_DB --quiet --eval "$@"; }
# mongo_js() { echo "$@" > $tmp_file.js  || die Cannot write to $tmp_file.js;  mongo $MONGODB/$MONGODB_DB --quiet $tmp_file.js; rm -f $tmp_file.js; }
ensure_kafka() { ps -ef | grep -q ' java .* [k]afka' && return; warn No kafka - starting...; sudo -i -u kafka ~kafka/bin/start-kafka; }
# ensure_mongo() { sudo docker ps| grep -q 'mongo' && return;     warn No mongo - starting...; sudo docker start $MONGODB_DB; }
kick_off() { local n=$1 cmd=$2 l=${2%.sh}; shift 2; l=${l%.go}; l=${l##*-}
	# put bg commands into their own process group - so that killing this script doesn't kill $cmd
	set -o monitor
	for (( i=0; i<n; i++ )); do
		e=; if (( i > 0 )); then e=$i; fi
		# echo $cmd $LOG_DIR/$l$e.log
		nohup $cmd >> $LOG_DIR/$l$e.log 2>&1 &
	done
}

[[ -n $start_kafka ]] && ensure_kafka
# ensure_mongo
mkdir -p $LOG_DIR || exit 3

[[ -z $1 ]] && set -- --up
res=0

while [[ -n $1 ]]; do
	arg=$1; shift
	if   [[ $arg == -h || $arg == --help ]]; then usage; break
	elif [[ $arg == --svc     ]]; then SERVICES=$1; shift
	elif [[ $arg == --kill || $arg == --ps ]]; then
		proc_re=
		for s in $SERVICES; do
			s=${s%:*}
			proc_re=$proc_re${proc_re:+|}${s#static-}
		done
		log ${arg#--}... $proc_re
		if   [[ $arg == --kill ]]; then
			if [[ $os == Darwin ]]; then	pkill -x "$proc_re"
			else				killall -r "^($proc_re)\$"
			fi
		elif [[ $arg == --ps   ]]; then
			pgrep -${p_flags}lx "$proc_re"; res=$?
		fi
	elif [[ $arg == --sleep   ]]; then log Sleeping to let logfiles be created...; sleep $1; shift
	elif [[ $arg == --tail    ]]; then log Tailing...; exec tail -f $LOG_DIR/*.log
	elif [[ $arg == --time    ]]; then
					ext=.log; if [[ -n $1 && $1 == [0-9]* ]]; then ext=.log.$1; shift; fi
					cd $LOG_DIR || exit 2
					(
						test -f scheduler.log && grep 'start:' scheduler$ext				|sort -n |tail -2
						test -f scheduler.log && grep 'completed' scheduler$ext				|sort -n |tail -2
						test -f scheduler.log && grep 'ERROR' scheduler$ext				|sort -n |tail -2
						test -f tracker.log   && grep Completed tracker$ext	|sort -n |tail -1
						test -f sender.log    && tail -qn 1 sender*$ext					|sort -n |tail -1
						test -f migrator.log  && tail -qn 1 migrator*$ext				|sort -n |tail -1
						test -f receiver.log  && tail -qn 20 receiver*$ext |grep Inserted		|sort -n |tail -1
					)|sort -n
	elif [[ $arg == --restart ]]; then set -- --kill --ps --rotate --up --sleep 1 --tail "$@"
	elif [[ $arg == --rotate  ]]; then date=$(date '+%Y%m%d-%H%M%S'); log Logs get extension .$date; for i in $LOG_DIR/*.log; do mv $i $i.$date; done
	elif [[ $arg == --seed    ]]; then
		when=0 coll= end_coll= stagger=0
		while [[ -n $1 ]]; do
			if [[ -z $coll ]]; then
				coll=$1
				end_coll=$coll
			elif [[ $1 =~ "+"[0-9]+ ]]; then
				stagger=${1#+}
			elif [[ $1 =~ :[0-9]+ ]]; then
				if [[ $os == Darwin ]]; then # it's a dev mac
					when=$(date -v+1M -v${1#:}S '+%s')  # get time for next minute and $1 secs
					when_pp=$(date -r $when)
				else
					when=$(date -d "1 minute -$(date '+%S') seconds ${1#:} seconds" '+%s')  # get time for next minute:00 add $1 secs
					when_pp=$(date -d'@'$when)
				fi
			elif [[ -d $ZEBEDEE_ROOT/collections/$1/complete ]]; then
				end_coll=$1
			else
				break
			fi
			shift
		done
		[[ -z $coll ]] && usage 1 >&2
		shopt -s extglob # for the regexes in the for() (remove leading zeroes otherwise bash thinks it is octal)
		count=0
		for (( c=${coll##test*(0)}; c <= ${end_coll##test*(0)}; c++ )); do
			len=${#c} col=${coll:0:-len}$c       # get prefix of $coll removing length of $c (i.e. coll=test0009,c=10 so len=2 col=test0010)
			# echo c=$c len=$len col=$col
			key=$(awk '$1=="'"$col"'"{print $3}' $ZEBEDEE_ROOT/collections/keys.txt); [[ -n $key ]] || die Cannot find key for collection $col
			echo '{"CollectionID":"'"$col"'","EncryptionKey":"'"$key"'","ScheduleTime":"'$when'"}'
			(( stagger > 0 )) && let when+=stagger
			let count++
		done
		warn scheduled $(colr $count) jobs from $(colr $when_pp) ... then $stagger sec apart
	elif [[ $arg == -m ]]; then
		sudo docker run -it --link $MONGODB_DB:mongo --rm mongo sh -c 'exec mongo "$MONGO_PORT_27017_TCP_ADDR:$MONGO_PORT_27017_TCP_PORT/'"$MONGODB_DB"'"'
	elif [[ $arg == -c ]]; then log Starting consumer...; kafka-console-consumer${k_ext} --bootstrap-server $KAFKA_ADDR --topic $sched_topic
	elif [[ $arg == -p ]]; then log Starting producer...; kafka-console-producer${k_ext} --broker-list $KAFKA_ADDR --topic $sched_topic
	elif [[ $arg == --resize ]]; then
		kafka-configs$k_ext --zookeeper $ZOOKEEPER --entity-type topics --entity-name uk.gov.ons.dp.web.complete-file --alter --add-config max.message.bytes=$KAFKA_MAX_BYTES
	elif [[ $arg == --groups ]]; then
		for i in $(kafka-consumer-groups$k_ext --bootstrap-server $KAFKA_ADDR --list);do kafka-consumer-groups$k_ext --new-consumer --bootstrap-server $KAFKA_ADDR --group $i --describe;done
	elif [[ $arg == --reset ]]; then
		log "Starting all-consuming consumer of $sched_group group for $sched_topic (Ctrl-C to quit)..."
		kafka-verifiable-consumer$k_ext --broker-list $KAFKA_ADDR --topic $sched_topic --group-id $sched_group --enable-autocommit --reset-policy latest --max-messages 1
	elif [[ $arg == --db       ]]; then sql "select * from schedule order by schedule_id" | perl -MTime::Piece -pE 's/(\d{13,})/$1 . " " . localtime->new(substr($1,0,-9))->datetime/eg;'
	elif [[ $arg == --db-wipe  ]]; then sql "delete from schedule"; mongo_js 'print(JSON.stringify(db.s3.remove({}),null,8));db.meta.remove({});'
	elif [[ $arg == --db-count ]]; then
		sql_count=$(psql $psql_args -U dp dp -t -c "select count(*) from schedule");         [[ -z $sql_count    ]] && die Could not count DB
		mongo_counts=$(mongo_js "print(db.s3.count() + ' ' + db.meta.count());"); [[ -z $mongo_counts ]] && die Bad mongo count
		log SQL: $sql_count s3: ${mongo_counts/ / meta: } total: $(( ${mongo_counts/ /+} ))
	elif [[ $arg == --db-index ]]; then
		mongo_js 'print(JSON.stringify(db.s3.createIndex({fileLocation:1}), null, 8),JSON.stringify(db.meta.createIndex({fileLocation:1,language:1}), null, 8))'
		# colls_js='var colls=["s3","meta"];' for_colls_js="$colls_js"'for(len=colls.length, i=0; i<len; i++)'
		# mongo_js "$for_colls_js"'{print(JSON.stringify(db[colls[i]].getIndexes(), null, 8))}'
		mongo_js 'print(JSON.stringify(db.s3.getIndexes(), null, 8),JSON.stringify(db.meta.getIndexes(), null, 8))'
	elif [[ $arg == --up ]]; then
		log Starting all services...
		for s in $SERVICES; do
			num=1
			if [[ ${s##*:} != $s ]]; then
				num=${s##*:}
				s=${s%:*}
			fi
			sh=~/bin/run-$s.sh
			if [[ -n $use_src ]]; then
				kick_off $num "go run $s/${s#static-}.go"
			elif [[ -x $sh ]]; then
				kick_off $num $sh
			else
				s=${s#static-}
				kick_off $num $s
			fi
		done
	else
		usage >&2
		die Bad arg: $arg
	fi
done

exit $res
# vim: tabstop=8 softtabstop=0 noexpandtab smarttab smartindent
